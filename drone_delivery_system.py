# -*- coding: utf-8 -*-
"""
===============================================================================
ğŸš DRONE PAKET TESLÄ°MAT SÄ°STEMÄ° - DEEP Q-NETWORK (DQN) Ä°LE PEKÄ°ÅTÄ°RMELÄ° Ã–ÄRENME
===============================================================================
Final Projesi - Ferhat Akalan

ğŸ“‹ PROJE AÃ‡IKLAMASI:
Bu proje, drone'larÄ±n ÅŸehir iÃ§i paket teslimatlarÄ±nda Deep Q-Network (DQN) algoritmasÄ± 
kullanarak optimal strateji Ã¶ÄŸrenmesini simÃ¼le eder.

ğŸ¯ SENARYO:
- 100 ÅŸarjlÄ± drone, 5x5 grid tabanlÄ± ortamda hareket eder
- Bir kargo deposu (ğŸŸ¢ yeÅŸil kare) saÄŸ alt kÃ¶ÅŸede bulunur
- Her episode'da 1-3 arasÄ± rastgele teslimat noktasÄ± (ğŸ”´ kÄ±rmÄ±zÄ± daireler) belirir
- Drone gÃ¶revi: kargo deposuna gidip kargo almalÄ± â†’ en yakÄ±n teslimat noktasÄ±na teslim etmeli

ğŸ§  DQN ALGORÄ°TMASI Ã–ZELLÄ°KLERÄ°:
- Experience Replay Buffer: GeÃ§miÅŸ deneyimlerden batch Ã¶ÄŸrenme (buffer_size=10,000)
- Target Network: KararlÄ± eÄŸitim iÃ§in ayrÄ± hedef aÄŸÄ± (update_freq=100 step)
- Epsilon-Greedy Exploration: KeÅŸif dengesi (1.0 â†’ 0.01)
- Optimizasyon edilmiÅŸ reward sistemi: Pozitif pekiÅŸtirme odaklÄ±
- 12 boyutlu state representation: Verimli durum temsili

ğŸ—ï¸ NEURAL NETWORK MÄ°MARÄ°SÄ°:
- Input Layer: 12 nÃ¶ron (state vector)
- Hidden Layer 1: 128 nÃ¶ron + ReLU + Dropout(0.2)
- Output Layer: 6 nÃ¶ron (Q-values for actions)

âš™ï¸ TEKNÄ°K DETAYLAR:
- PyTorch ile DQN implementasyonu
- PyQt5 ile interaktif gÃ¶rsel arayÃ¼z
- Thread-based eÄŸitim (UI donmamasÄ± iÃ§in)
- Real-time neural network gÃ¶rselleÅŸtirmesi
- Model kaydet/yÃ¼kle fonksiyonlarÄ± (.pth format)

Bu proje, reinforcement learning'in gerÃ§ek dÃ¼nya uygulamalarÄ±na bir Ã¶rnektir.
Drone'lar bu algoritmalarla otonom olarak karar verebilir hale gelir.
"""

# =====================
# KÃœTÃœPHANE Ä°MPORTLARI
# =====================
import sys
import os
import random
import pickle
import numpy as np

# PyTorch Deep Learning kÃ¼tÃ¼phaneleri
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque

# PyQt5 GUI kÃ¼tÃ¼phaneleri
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QPushButton, 
                             QVBoxLayout, QHBoxLayout, QLabel, QSpinBox, 
                             QDoubleSpinBox, QGroupBox, QGridLayout, QFileDialog, 
                             QMessageBox, QComboBox, QSlider)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread
from PyQt5.QtGui import QPainter, QColor, QBrush, QPen, QFont, QIcon, QPixmap

# =====================
# ORTAM (ENVIRONMENT) SINIFI
# =====================
class DroneDeliveryEnv:
    """
    ========================================================================
    DRONE TESLÄ°MAT ORTAMI (REINFORCEMENT LEARNING ENVIRONMENT)
    ========================================================================
    OpenAI Gym benzeri ortam yapÄ±sÄ±:
    - Grid tabanlÄ± 2D dÃ¼nya (5x5, 6x6, 7x7 boyutlarÄ±nda)
    - State: 12 boyutlu vektÃ¶r (drone pozisyonu, kargo durumu, hedef bilgisi)
    - Action: 6 farklÄ± eylem (hareket, kargo alma/bÄ±rakma, kalkÄ±ÅŸ/iniÅŸ)
    - Reward: Optimizasyon edilmiÅŸ Ã¶dÃ¼l sistemi
    
    GÃ¶rsel Kodlama:
    - ğŸŸ¢ YeÅŸil: Kargo deposu
    - ğŸ”´ KÄ±rmÄ±zÄ±: Teslimat noktalarÄ±
    - ğŸ”µ Mavi: Drone
    - ğŸ”‹ Batarya seviyesi
    """
    def __init__(self, grid_size=5, max_steps=100, n_deliveries=1):
        """
        Ortam parametrelerini baÅŸlat
        
        Args:
            grid_size (int): Grid boyutu (3-7 arasÄ±)
            max_steps (int): Maksimum adÄ±m sayÄ±sÄ±
            n_deliveries (int): Teslimat noktasÄ± sayÄ±sÄ± (1-3 arasÄ±)
        """
        # === TEMEL PARAMETRELER ===
        self.grid_size = grid_size
        self.max_steps = max_steps
        self.n_deliveries = n_deliveries
        
        # === EYLEM UZAYI ===
        # 6 farklÄ± eylem: hareket (4) + kargo iÅŸlemi (1) + uÃ§uÅŸ kontrolÃ¼ (1)
        self.action_space_n = 6
        # Eylem aÃ§Ä±klamalarÄ±:
        # 0: AÅŸaÄŸÄ± â¬‡ï¸, 1: SaÄŸa â¡ï¸, 2: YukarÄ± â¬†ï¸, 3: Sola â¬…ï¸
        # 4: Kargo Al/BÄ±rak ğŸ“¦, 5: Kalk/Ä°n ğŸ›«ğŸ›¬
        
        # === SABÄ°T TESLÄ°MAT NOKTALARÄ± ===
        # Grid'in 4 kÃ¶ÅŸesi (Taxi-v3 problemi benzeri)
        self.fixed_delivery_points = [
            np.array([0, 0]),                    # Sol Ã¼st kÃ¶ÅŸe
            np.array([0, self.grid_size-1]),     # SaÄŸ Ã¼st kÃ¶ÅŸe
            np.array([self.grid_size-1, 0]),     # Sol alt kÃ¶ÅŸe
            np.array([self.grid_size-1, self.grid_size-1])  # SaÄŸ alt kÃ¶ÅŸe
        ]
        
        # === BATARYA TÃœKETÄ°M ORANLARI ===
        self.move_battery_cost = 1      # Normal hareket: 1 birim
        self.takeoff_battery_cost = 3   # KalkÄ±ÅŸ: 3 birim (azaltÄ±ldÄ±)
        self.landing_battery_cost = 3   # Ä°niÅŸ: 3 birim (azaltÄ±ldÄ±)
        
        # OrtamÄ± ilk duruma getir
        self.reset()

    def reset(self):
        """
        ===============================================
        ORTAMI SIFIRLA (YENÄ° EPÄ°SODE BAÅLAT)
        ===============================================
        Her episode baÅŸÄ±nda ortamÄ± baÅŸlangÄ±Ã§ durumuna getirir.
        Rastgele drone pozisyonu ve teslimat noktalarÄ± oluÅŸturur.
        
        Returns:
            np.ndarray: BaÅŸlangÄ±Ã§ state vektÃ¶rÃ¼ (12 boyutlu)
        """
        # === DRONE BAÅLANGIÃ‡ POZÄ°SYONU ===
        # Drone'u grid Ã¼zerinde rastgele konumlandÄ±r
        self.drone_pos = np.array([
            random.randint(0, self.grid_size-1),
            random.randint(0, self.grid_size-1)
        ])
        
        # === KARGO DEPOSU POZÄ°SYONU ===
        # Kargo deposu her zaman saÄŸ alt kÃ¶ÅŸede (sabit)
        self.cargo_depot_pos = np.array([self.grid_size-1, self.grid_size-1])
        
        # === TESLÄ°MAT NOKTALARINI SEÃ‡ ===
        # Her episode'da 1-3 arasÄ± rastgele teslimat noktasÄ±
        self.n_deliveries = random.randint(1, 3)
        
        # Kargo deposu dÄ±ÅŸÄ±ndaki kÃ¶ÅŸelerden teslimat noktalarÄ±nÄ± seÃ§
        available_indices = [i for i in range(len(self.fixed_delivery_points)) 
                           if not np.array_equal(self.fixed_delivery_points[i], self.cargo_depot_pos)]
        chosen_indices = random.sample(available_indices, self.n_deliveries)
        self.delivery_points = [self.fixed_delivery_points[i].copy() for i in chosen_indices]
        self.delivery_indices = chosen_indices
        
        # === DRONE DURUMU ===
        self.has_cargo = False          # Kargo taÅŸÄ±yor mu?
        self.battery = 100              # Batarya seviyesi (100% full)
        self.steps = 0                  # AdÄ±m sayacÄ±
        self.delivered = [False] * len(self.delivery_points)  # Teslimat durumlarÄ±
        self.done = False               # Episode bitmiÅŸ mi?
        self.is_flying = False          # Drone uÃ§uyor mu?
        
        # === GÃ–RSEL ANÄ°MASYON DEÄÄ°ÅKENLERÄ° ===
        self.landing_state = "landed"           # Ä°niÅŸ/kalkÄ±ÅŸ durumu
        self.landing_animation_step = 0         # Animasyon adÄ±mÄ±
        
        # === Ã–DÃœL TAKÄ°P DEÄÄ°ÅKENLERÄ° ===
        self.last_reward = 0            # Son adÄ±mdaki Ã¶dÃ¼l
        self.total_reward = 0           # Episode toplam Ã¶dÃ¼lÃ¼
        
        return self.get_state()         # BaÅŸlangÄ±Ã§ state'ini dÃ¶ndÃ¼r

    def get_state(self):
        """
        ===============================================
        STATE REPRESENTATION (DURUM TEMSÄ°LÄ°)
        ===============================================
        DQN iÃ§in optimize edilmiÅŸ 12 boyutlu state vektÃ¶rÃ¼ oluÅŸturur.
        Bu vektÃ¶r, neural network'Ã¼n karar vermesi iÃ§in gerekli tÃ¼m bilgiyi iÃ§erir.
        
        State VektÃ¶rÃ¼ Ä°Ã§eriÄŸi:
        [0-1]: Drone pozisyonu (x, y) - normalize edilmiÅŸ
        [2-3]: Drone durumlarÄ± (kargo_var_mÄ±, uÃ§uyor_mu) - binary
        [4]:   Batarya seviyesi - normalize edilmiÅŸ (0-1)
        [5-6]: Kargo deposu pozisyonu - normalize edilmiÅŸ
        [7-8]: Hedef pozisyonu - normalize edilmiÅŸ
        [9]:   Hedefe mesafe - normalize edilmiÅŸ
        [10]:  Teslimat tamamlanma oranÄ±
        [11]:  Kalan adÄ±m oranÄ±
        
        Returns:
            np.ndarray: 12 boyutlu float32 state vektÃ¶rÃ¼
        """
        state = np.zeros(12, dtype=np.float32)
        
        # === DRONE POZÄ°SYONU (Normalize edilmiÅŸ) ===
        state[0] = self.drone_pos[0] / (self.grid_size - 1) if self.grid_size > 1 else 0
        state[1] = self.drone_pos[1] / (self.grid_size - 1) if self.grid_size > 1 else 0
        
        # === DRONE DURUMLARI (Binary) ===
        state[2] = float(self.has_cargo)    # 1.0 if kargo var, 0.0 if yok
        state[3] = float(self.is_flying)    # 1.0 if uÃ§uyor, 0.0 if yerde
        
        # === BATARYA SEVÄ°YESÄ° (Normalize edilmiÅŸ) ===
        state[4] = self.battery / 100.0     # 0.0 - 1.0 arasÄ±
        
        # === KARGO DEPOSU POZÄ°SYONU (Normalize edilmiÅŸ) ===
        state[5] = self.cargo_depot_pos[0] / (self.grid_size - 1) if self.grid_size > 1 else 0
        state[6] = self.cargo_depot_pos[1] / (self.grid_size - 1) if self.grid_size > 1 else 0
        
        # === HEDEF BELÄ°RLEME LOJÄ°ÄÄ° ===
        target_x, target_y = 0, 0
        if not self.has_cargo:
            # Kargo yoksa â†’ Kargo deposuna git
            target_x = self.cargo_depot_pos[0] / (self.grid_size - 1) if self.grid_size > 1 else 0
            target_y = self.cargo_depot_pos[1] / (self.grid_size - 1) if self.grid_size > 1 else 0
        else:
            # Kargo varsa â†’ En yakÄ±n teslim edilmemiÅŸ noktaya git
            min_dist = float('inf')
            closest_point = None
            for i, point in enumerate(self.delivery_points):
                if not self.delivered[i]:
                    # Manhattan distance hesapla
                    dist = abs(self.drone_pos[0] - point[0]) + abs(self.drone_pos[1] - point[1])
                    if dist < min_dist:
                        min_dist = dist
                        closest_point = point
            
            if closest_point is not None:
                target_x = closest_point[0] / (self.grid_size - 1) if self.grid_size > 1 else 0
                target_y = closest_point[1] / (self.grid_size - 1) if self.grid_size > 1 else 0
        
        # === HEDEF BÄ°LGÄ°LERÄ° ===
        state[7] = target_x
        state[8] = target_y
        
        # === HEDEFE MESAFE (Normalize edilmiÅŸ) ===
        distance_to_target = abs(self.drone_pos[0] - target_x * (self.grid_size - 1)) + \
                           abs(self.drone_pos[1] - target_y * (self.grid_size - 1))
        state[9] = distance_to_target / (2 * self.grid_size)  # Maksimum mesafe ile normalize et
        
        # === TESLÄ°MAT DURUMU ===
        state[10] = sum(self.delivered) / len(self.delivered) if self.delivered else 0
        
        # === ZAMAN BÄ°LGÄ°SÄ° ===
        state[11] = (self.max_steps - self.steps) / self.max_steps  # Kalan adÄ±m oranÄ±
        
        return state

    def step(self, action):
        """
        Drone'a verilen eylemi uygular ve ortamÄ± bir adÄ±m ilerletir.
        Args:
            action (int):
                0: AÅŸaÄŸÄ±, 1: SaÄŸa, 2: YukarÄ±, 3: Sola
                4: Kargo Al/BÄ±rak (Yerdeyken kargo al veya teslim et)
                5: Kalk/Ä°n (Take off/landing, uÃ§uÅŸ durumunu deÄŸiÅŸtirir)
        Returns:
            tuple: (next_state, reward, done, info)
                next_state: Yeni durumun hashlenmiÅŸ temsili
                reward: Bu adÄ±mda alÄ±nan Ã¶dÃ¼l/ceza
                done: Senaryo tamamlandÄ± mÄ±?
                info: Ek bilgi (Ã¶r. neden bitti, hangi eylem yapÄ±ldÄ±)
        Drone kargo almak ve bÄ±rakmak iÃ§in landing durumunda olmalÄ±. Havadayken kargo bÄ±rakÄ±lamaz alÄ±namaz.
        """
        if self.done:
            return self.get_state(), 0, True, {"info": "Senaryo zaten tamamlanmÄ±ÅŸ."}
        
        # BaÅŸlangÄ±Ã§ durumu
        old_pos = self.drone_pos.copy()
        reward = 0
        info = {}
        action_emojis = {
            0: 'â¬‡ï¸',  # AÅŸaÄŸÄ±
            1: 'â¡ï¸',  # SaÄŸa
            2: 'â¬†ï¸',  # YukarÄ±
            3: 'â¬…ï¸',  # Sola
            4: 'ğŸ“¦',  # Kargo Al/BÄ±rak
            5: 'ğŸ›«/ğŸ›¬',  # Kalk/Ä°n
        }
        action_names = {
            0: 'AÅŸaÄŸÄ± hareket',
            1: 'SaÄŸa hareket',
            2: 'YukarÄ± hareket',
            3: 'Sola hareket',
            4: 'Kargo Al/BÄ±rak',
            5: 'Kalk/Ä°n',
        }
        # --- Eylem tipine gÃ¶re Ã¶dÃ¼l/ceza ---
        if action <= 3:  # Hareket eylemleri
            if not self.is_flying:
                reward -= 0.5  # Daha az ceza, Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±r
                info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Drone yerdeyken hareket edemez! Ã–nce kalkÄ±ÅŸ yapÄ±n."
            else:
                if action == 0:
                    self.drone_pos[0] = min(self.drone_pos[0] + 1, self.grid_size - 1)
                elif action == 1:
                    self.drone_pos[1] = min(self.drone_pos[1] + 1, self.grid_size - 1)
                elif action == 2:
                    self.drone_pos[0] = max(self.drone_pos[0] - 1, 0)
                elif action == 3:
                    self.drone_pos[1] = max(self.drone_pos[1] - 1, 0)
                if np.array_equal(old_pos, self.drone_pos):
                    reward -= 1  # Daha az ceza
                    info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Hareket edilemedi."
                else:
                    reward -= 0.2  # KÃ¼Ã§Ã¼k hareket cezasÄ±
                    self.battery -= self.move_battery_cost
                    info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action})"
        elif action == 4:  # Kargo Al/BÄ±rak
            if self.is_flying:
                reward -= 2  # Daha az ceza
                info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Drone havadayken kargo alÄ±namaz/bÄ±rakÄ±lamaz! Ã–nce iniÅŸ yapÄ±n."
            else:
                if np.array_equal(self.drone_pos, self.cargo_depot_pos) and not self.has_cargo:
                    self.has_cargo = True
                    reward += 200  # Kargo alma Ã¶dÃ¼lÃ¼ artÄ±rÄ±ldÄ±
                    info["action"] = f"{action_emojis[action]} Kargo alÄ±ndÄ± (action={action})"
                elif self.has_cargo:
                    delivered_any = False
                    for i, delivery_point in enumerate(self.delivery_points):
                        if np.array_equal(self.drone_pos, delivery_point) and not self.delivered[i]:
                            self.delivered[i] = True
                            self.has_cargo = False
                            reward += 500  # Teslimat Ã¶dÃ¼lÃ¼ artÄ±rÄ±ldÄ±
                            info["action"] = f"{action_emojis[action]} {i+1}. teslimat tamamlandÄ± (action={action})"
                            delivered_any = True
                            break
                    if not delivered_any:
                        reward -= 5  # YanlÄ±ÅŸ yerde teslimat cezasÄ±
                        info["action"] = f"{action_emojis[action]} YanlÄ±ÅŸ yerde teslimat (action={action})"
                else:
                    reward -= 5  # YanlÄ±ÅŸ yerde kargo alma/bÄ±rakma cezasÄ±
                    info["action"] = f"{action_emojis[action]} Burada kargo alÄ±namaz/bÄ±rakÄ±lamaz (action={action})"
        elif action == 5:  # Kalk/Ä°n
            if not self.is_flying:
                self.is_flying = True
                self.landing_state = "taking_off"
                self.landing_animation_step = 0
                reward -= 0.5  # Daha az ceza
                info["action"] = f"ğŸ›« KalkÄ±ÅŸ (action={action})"
                self.battery -= self.takeoff_battery_cost
            else:
                self.is_flying = False
                self.landing_state = "landing"
                self.landing_animation_step = 0
                reward -= 0.5  # Daha az ceza
                info["action"] = f"ğŸ›¬ Ä°niÅŸ (action={action})"
                self.battery -= self.landing_battery_cost

        # --- Hedefe yaklaÅŸma/uzaklaÅŸma Ã¶dÃ¼l/ceza ---
        target_pos = None
        if not self.has_cargo and not all(self.delivered):
            target_pos = self.cargo_depot_pos
            # Kargo almak iÃ§in ek motivasyon: kargo deposuna yaklaÅŸtÄ±kÃ§a bonus
            depot_dist = np.sum(np.abs(self.drone_pos - self.cargo_depot_pos))
            if depot_dist <= 1 and not self.is_flying:
                reward += 15  # Kargo deposuna yakÄ±n ve yerdeyse bonus
        elif self.has_cargo:
            min_dist = float('inf')
            for i, point in enumerate(self.delivery_points):
                if not self.delivered[i]:
                    dist = np.sum(np.abs(self.drone_pos - point))
                    if dist < min_dist:
                        min_dist = dist
                        target_pos = point
                        
        if target_pos is not None:
            old_dist = np.sum(np.abs(old_pos - target_pos))
            new_dist = np.sum(np.abs(self.drone_pos - target_pos))
            if self.is_flying and new_dist < old_dist:
                reward += 10  # Hedefe yaklaÅŸma Ã¶dÃ¼lÃ¼ artÄ±rÄ±ldÄ±
            elif self.is_flying and new_dist > old_dist:
                reward -= 1  # Hedeften uzaklaÅŸma cezasÄ± azaltÄ±ldÄ±
            if np.array_equal(self.drone_pos, target_pos):
                if not self.is_flying and action == 4:
                    reward += 10  # DoÄŸru yerde doÄŸru eylem bonusu
                elif self.is_flying and action == 5:
                    reward += 5  # DoÄŸru yerde iniÅŸ bonusu

        # --- Batarya kontrolÃ¼ ---
        if self.battery <= 0:
            reward -= 50  # Batarya biterse ceza azaltÄ±ldÄ±
            self.battery = 0
            self.done = True
            info["done_reason"] = "Batarya bitti"

        # --- AdÄ±m sÄ±nÄ±rÄ± ---
        self.steps += 1
        if self.steps >= self.max_steps:
            reward -= 20  # Maksimum adÄ±m cezasÄ± azaltÄ±ldÄ±
            self.done = True
            info["done_reason"] = "Maksimum adÄ±m sayÄ±sÄ±na ulaÅŸÄ±ldÄ±"

        # --- TÃ¼m teslimatlar tamamlandÄ±ysa ---
        if all(self.delivered):
            remaining_battery_bonus = self.battery
            reward += 500 + remaining_battery_bonus  # Ã‡ok bÃ¼yÃ¼k Ã¶dÃ¼l ve kalan batarya bonusu
            self.done = True
            info["done_reason"] = f"TÃ¼m teslimatlar tamamlandÄ±! Kalan batarya: %{self.battery}"

        # Ä°niÅŸ/kalkÄ±ÅŸ animasyon durumlarÄ±nÄ± gÃ¼ncelle
        # Bu adÄ±mlar, gÃ¶rsel arayÃ¼zde animasyonun dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.
        if self.landing_state == "taking_off":
            self.landing_animation_step += 1
            if self.landing_animation_step >= 3:  # 3 adÄ±mda tamamlanan kalkÄ±ÅŸ animasyonu
                self.landing_state = "flying"
        elif self.landing_state == "landing":
            self.landing_animation_step += 1
            if self.landing_animation_step >= 3:  # 3 adÄ±mda tamamlanan iniÅŸ animasyonu
                self.landing_state = "landed"
        
        self.last_reward = reward  # Son Ã¶dÃ¼l bilgisini gÃ¼ncelle
        self.total_reward += reward  # Toplam Ã¶dÃ¼lÃ¼ gÃ¼ncelle
        # Son aksiyon bilgisini ortamda sakla
        self.last_action_info = info.get("action", "-")
        return self.get_state(), reward, self.done, info # Yeni durum, Ã¶dÃ¼l, bÃ¶lÃ¼m durumu ve ek bilgiyi dÃ¶ndÃ¼r.
# =====================
# DEEP Q-NETWORK (DQN) NEURAL NETWORK
# =====================
class DQN(nn.Module):
    """
    ========================================================================
    DEEP Q-NETWORK SINIR AÄI (PYTORCH IMPLEMENTATION)
    ========================================================================
    DQN algoritmasÄ±nÄ±n kalbi olan neural network yapÄ±sÄ±.
    
    Mimari:
    - Input Layer: 12 nÃ¶ron (state vector boyutu)
    - Hidden Layer 1: 128 nÃ¶ron + ReLU activation
    - Output Layer: 6 nÃ¶ron (action space boyutu)
    
    Ã–zellikler:
    - Dropout: %20 overfitting Ã¶nleme
    - ReLU Activation: Non-linearity iÃ§in
    
    Bu aÄŸ, state'i alÄ±r ve her action iÃ§in Q-value tahmin eder.
    En yÃ¼ksek Q-value'ya sahip action seÃ§ilir (greedy policy).
    """
    def __init__(self, state_size, action_size, hidden_size=128):
        """
        Neural network katmanlarÄ±nÄ± tanÄ±mla
        
        Args:
            state_size (int): Input boyutu (12)
            action_size (int): Output boyutu (6) 
            hidden_size (int): Hidden layer boyutu (128)
        """
        super(DQN, self).__init__()
        
        # === KATMAN TANIMLARI ===
        self.fc1 = nn.Linear(state_size, hidden_size)      # Ä°lk gizli katman
        self.fc2 = nn.Linear(hidden_size, hidden_size)     # Ä°kinci gizli katman
        self.fc3 = nn.Linear(hidden_size, action_size)     # Ã‡Ä±kÄ±ÅŸ katmanÄ±
        
        # === REGULARÄ°ZASYON ===
        self.dropout = nn.Dropout(0.2)  # %20 dropout ile overfitting Ã¶nleme
        
    def forward(self, x):
        """
        Ä°leri geÃ§iÅŸ (forward pass) - state'den Q-values'a
        
        Args:
            x (torch.Tensor): Input state tensor
            
        Returns:
            torch.Tensor: Q-values for all actions
        """
        # Input â†’ Hidden Layer 1 (ReLU activation)
        x = F.relu(self.fc1(x))
        
        # Dropout uygula (sadece training sÄ±rasÄ±nda)
        x = self.dropout(x)
        
        # Hidden Layer 1 â†’ Hidden Layer 2 (ReLU activation)
        x = F.relu(self.fc2(x))
        
        # Hidden Layer 2 â†’ Output (Linear, no activation)
        x = self.fc3(x)
        
        return x  # Q-values for all 6 actions

# =====================
# DQN AGENT (REINFORCEMENT LEARNING AGENT)
# =====================
class DQNAgent:
    """
    ========================================================================
    DEEP Q-NETWORK AGENT - PEKÄ°ÅTÄ°RMELÄ° Ã–ÄRENME AJANI
    ========================================================================
    DQN algoritmasÄ±nÄ±n tam implementasyonu. Bu sÄ±nÄ±f:
    
    ğŸ§  Ã–ÄŸrenme MekanizmalarÄ±:
    - Experience Replay: GeÃ§miÅŸ deneyimlerden batch Ã¶ÄŸrenme
    - Target Network: KararlÄ± eÄŸitim iÃ§in ayrÄ± hedef aÄŸÄ±
    - Epsilon-Greedy: Exploration vs Exploitation dengesi
    
    ğŸ“Š Hiperparametreler:
    - Learning Rate: 0.001 (Adam optimizer)
    - Gamma: 0.99 (discount factor)
    - Epsilon: 1.0 â†’ 0.01 (exploration decay)
    - Batch Size: 32 (mini-batch learning)
    - Buffer Size: 10,000 (experience replay)
    
    ğŸ¯ Algoritma AdÄ±mlarÄ±:
    1. Action selection (epsilon-greedy)
    2. Experience storage (replay buffer)
    3. Batch learning (Q-learning update)
    4. Target network update (stability)
    """
    def __init__(self, env, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.998, 
                 min_epsilon=0.01, buffer_size=20000, batch_size=32, target_update=250):
        """
        DQN Agent'Ä± baÅŸlat
        
        Args:
            env: Drone delivery environment
            lr (float): Learning rate (0.001)
            gamma (float): Discount factor (0.99)
            epsilon (float): Initial exploration rate (1.0)
            epsilon_decay (float): Exploration decay rate (0.995)
            min_epsilon (float): Minimum exploration rate (0.01)
            buffer_size (int): Experience replay buffer size (10,000)
            batch_size (int): Mini-batch size for learning (32)
            target_update (int): Target network update frequency (100)
        """
        # === TEMEL PARAMETRELER ===
        self.env = env
        self.state_size = 12                    # Optimize edilmiÅŸ state size
        self.action_size = env.action_space_n   # 6 action
        
        # === Ã–ÄRENME HÄ°PERPARAMETRELERÄ° ===
        self.lr = lr                    # Learning rate
        self.gamma = gamma              # Discount factor (gelecek Ã¶dÃ¼l oranÄ±)
        self.epsilon = epsilon          # Exploration rate (keÅŸif oranÄ±)
        self.epsilon_decay = epsilon_decay  # Epsilon azalma oranÄ± (biraz daha yavaÅŸ decay)
        self.min_epsilon = min_epsilon  # Minimum epsilon
        self.batch_size = batch_size    # Mini-batch boyutu
        self.target_update = target_update  # Target network gÃ¼ncelleme sÄ±klÄ±ÄŸÄ±
        
        # === DEVICE SEÃ‡Ä°MÄ° (GPU/CPU) ===
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  Using device: {self.device}")
        
        # === NEURAL NETWORKS ===
        # Ana network: EÄŸitilen, gradient alan network
        self.q_network = DQN(self.state_size, self.action_size).to(self.device)
        # Target network: KararlÄ± target Q-values iÃ§in ayrÄ± network
        self.target_network = DQN(self.state_size, self.action_size).to(self.device)
        
        # === OPTIMIZER ===
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        # === EXPERIENCE REPLAY BUFFER ===
        # GeÃ§miÅŸ deneyimleri saklar: (state, action, reward, next_state, done)
        self.memory = deque(maxlen=buffer_size)
        
        # === SAYAÃ‡LAR ===
        self.step_count = 0     # Toplam adÄ±m sayÄ±sÄ±
        self.update_count = 0   # GÃ¼ncelleme sayÄ±sÄ±
        
        # Ä°lk target network gÃ¼ncellemesi (aÄŸÄ±rlÄ±klarÄ± kopyala)
        self.update_target_network()
        
    def select_action(self, state, training=True):
        """
        ===============================================
        EPSILON-GREEDY ACTION SELECTION
        ===============================================
        DQN'in kalbi: Exploration vs Exploitation dengesi
        
        Epsilon-Greedy Stratejisi:
        - Epsilon olasÄ±lÄ±kla: Rastgele action seÃ§ (EXPLORATION) ğŸ²
        - (1-Epsilon) olasÄ±lÄ±kla: En iyi Q-value'lu action seÃ§ (EXPLOITATION) ğŸ¯
        
        EÄŸitim ilerledikÃ§e epsilon azalÄ±r:
        1.0 â†’ 0.01 (keÅŸiften istismara geÃ§iÅŸ)
        
        Args:
            state (np.ndarray): Mevcut durum vektÃ¶rÃ¼
            training (bool): EÄŸitim modu mu? (epsilon kullanÄ±lsÄ±n mÄ±?)
            
        Returns:
            int: SeÃ§ilen action (0-5 arasÄ±)
        """
        if training and random.random() < self.epsilon:
            # EXPLORATION: Rastgele action seÃ§
            return random.randint(0, self.action_size - 1)
        else:
            # EXPLOITATION: En iyi Q-value'lu action seÃ§
            with torch.no_grad():  # Gradient hesaplama, sadece inference
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()  # En yÃ¼ksek Q-value'nun indeksi
    
    def remember(self, state, action, reward, next_state, done):
        """
        ===============================================
        EXPERIENCE REPLAY BUFFER'A DENEYÄ°M EKLE
        ===============================================
        Her adÄ±mdan sonra deneyimi buffer'a kaydeder.
        Bu deneyimler daha sonra batch halinde Ã¶ÄŸrenme iÃ§in kullanÄ±lÄ±r.
        
        Experience Tuple: (s, a, r, s', done)
        - s: Mevcut state
        - a: YapÄ±lan action  
        - r: AlÄ±nan reward
        - s': Sonraki state
        - done: Episode bitmiÅŸ mi?
        
        Args:
            state: Mevcut durum
            action: YapÄ±lan eylem
            reward: AlÄ±nan Ã¶dÃ¼l
            next_state: Sonraki durum
            done: Episode tamamlandÄ± mÄ±?
        """
        self.memory.append((state, action, reward, next_state, done))
    
    def learn(self, state, action, reward, next_state, done):
        """
        ===============================================
        Ã–ÄRENME DÃ–NGÃœSÃœ (MAIN LEARNING LOOP)
        ===============================================
        Her adÄ±mdan sonra Ã§aÄŸrÄ±lan ana Ã¶ÄŸrenme fonksiyonu:
        
        1. Deneyimi buffer'a kaydet
        2. Yeterli deneyim varsa batch Ã¶ÄŸrenme yap
        3. Target network'Ã¼ belirli aralÄ±klarla gÃ¼ncelle
        
        Bu fonksiyon DQN'in Ã¶ÄŸrenme hÄ±zÄ±nÄ± kontrol eder.
        
        Args:
            state: Ã–nceki durum
            action: YapÄ±lan eylem
            reward: AlÄ±nan Ã¶dÃ¼l
            next_state: Yeni durum
            done: Episode bitti mi?
        """
        # 1. Deneyimi buffer'a kaydet
        self.remember(state, action, reward, next_state, done)
        
        self.step_count += 1
        
        # 2. Her 4 adÄ±mda bir batch Ã¶ÄŸrenme yap (computational efficiency)
        if len(self.memory) > self.batch_size and self.step_count % 4 == 0:
            self.replay()
            
        # 3. Target network'Ã¼ belirli aralÄ±klarla gÃ¼ncelle (stability)
        if self.step_count % self.target_update == 0:
            self.update_target_network()
    
    def replay(self):
        """Experience replay ile batch Ã¶ÄŸrenme"""
        if len(self.memory) < self.batch_size:
            return
            
        # Random batch seÃ§
        batch = random.sample(self.memory, self.batch_size)
        
        # Her state'in aynÄ± boyutta olduÄŸundan emin ol
        states_list = []
        actions_list = []
        rewards_list = []
        next_states_list = []
        dones_list = []
        
        for experience in batch:
            state, action, reward, next_state, done = experience
            
            # State'lerin numpy array olduÄŸundan ve doÄŸru boyutta olduÄŸundan emin ol
            if isinstance(state, np.ndarray) and state.shape == (12,):  # 12 boyutlu state
                states_list.append(state)
            else:
                # EÄŸer state doÄŸru formatta deÄŸilse, varsayÄ±lan state oluÅŸtur
                states_list.append(np.zeros(12))
                
            if isinstance(next_state, np.ndarray) and next_state.shape == (12,):
                next_states_list.append(next_state)
            else:
                next_states_list.append(np.zeros(12))
                
            actions_list.append(action)
            rewards_list.append(reward)
            dones_list.append(done)
        
        # Numpy array'lere dÃ¶nÃ¼ÅŸtÃ¼r
        states = np.array(states_list, dtype=np.float32)
        actions = np.array(actions_list, dtype=np.int64)
        rewards = np.array(rewards_list, dtype=np.float32)
        next_states = np.array(next_states_list, dtype=np.float32)
        dones = np.array(dones_list, dtype=bool)
        
        # Tensor'lara dÃ¶nÃ¼ÅŸtÃ¼r
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Next Q values (Target network ile)
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Loss hesapla ve backpropagation
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        self.update_count += 1
    
    def update_target_network(self):
        """Target network'u main network ile gÃ¼ncelle"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def decay_epsilon(self):
        """Epsilon'u azalt"""
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
    
    def save_model(self, filename):
        """Modeli kaydet"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count,
            'update_count': self.update_count
        }, filename)
    
    def load_model(self, filename):
        """Modeli yÃ¼kle"""
        checkpoint = torch.load(filename, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint.get('epsilon', self.min_epsilon)
        self.step_count = checkpoint.get('step_count', 0)
        self.update_count = checkpoint.get('update_count', 0)

# =====================
# EÄŸitim Thread'i (PyQt5)
# =====================
class TrainingThread(QThread): # PyQt5 QThread sÄ±nÄ±fÄ±ndan miras alÄ±r, bÃ¶ylece arayÃ¼z donmadan eÄŸitim yapÄ±labilir.
    progress = pyqtSignal(int, float, float, float)  # episode, reward, steps, epsilon -> EÄŸitim ilerlemesini bildiren sinyal.
    finished = pyqtSignal(list, list) # EÄŸitim bittiÄŸinde Ã¶dÃ¼l ve adÄ±m listelerini gÃ¶nderen sinyal.
    state_update = pyqtSignal() # Ortam durumunun gÃ¼ncellenmesi gerektiÄŸini bildiren sinyal (gÃ¶rsel arayÃ¼z iÃ§in).
    def __init__(self, env, agent, episodes, update_interval=10, mode="fast", delay=0.1): # "ansi" -> "fast"
        super().__init__()
        self.env = env # EÄŸitim ortamÄ±.
        self.agent = agent # EÄŸitilecek ajan.
        self.episodes = episodes # Toplam eÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ±.
        self.running = True # EÄŸitimin devam edip etmediÄŸini kontrol eden bayrak.
        self.update_interval = update_interval # fast modunda ne sÄ±klÄ±kta arayÃ¼zÃ¼n gÃ¼ncelleneceÄŸi.
        self.mode = mode  # 'human' (canlÄ± izleme) veya 'fast' (hÄ±zlÄ± eÄŸitim).
        self.delay = delay  # 'human' modunda adÄ±mlar arasÄ± gecikme (saniye).
    def run(self):
        # EÄŸitim dÃ¶ngÃ¼sÃ¼ (her episode iÃ§in)
        rewards_per_episode = [] # Her bÃ¶lÃ¼mdeki toplam Ã¶dÃ¼lÃ¼ saklar.
        steps_per_episode = [] # Her bÃ¶lÃ¼mdeki adÄ±m sayÄ±sÄ±nÄ± saklar.
        for episode in range(self.episodes):
            if not self.running: # EÄŸer durdurma sinyali geldiyse eÄŸitimi sonlandÄ±r.
                break
            state = self.env.reset() # OrtamÄ± sÄ±fÄ±rla.
            total_reward = 0 # Bu bÃ¶lÃ¼mdeki toplam Ã¶dÃ¼l.
            done = False # BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi.
            step_counter = 0 # Bu bÃ¶lÃ¼mdeki adÄ±m sayÄ±sÄ±.
            self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            while not done and self.running: # BÃ¶lÃ¼m bitene kadar veya durdurma sinyali gelene kadar devam et.
                action = self.agent.select_action(state, training=True) # Ajan bir eylem seÃ§er.
                next_state, reward, done, info = self.env.step(action) # Ortamda eylemi uygula.
                self.agent.learn(state, action, reward, next_state, done) # Ajan Ã¶ÄŸrenir.
                state = next_state # Durumu gÃ¼ncelle.
                total_reward += reward # Toplam Ã¶dÃ¼lÃ¼ gÃ¼ncelle.
                step_counter += 1
                if self.mode == "human": # EÄŸer 'human' modundaysa
                    self.state_update.emit() # ArayÃ¼zÃ¼ her adÄ±mda gÃ¼ncelle.
                    QThread.msleep(int(self.delay * 1000)) # Belirlenen sÃ¼re kadar bekle.
                elif self.mode == "fast" and step_counter % self.update_interval == 0: # EÄŸer 'fast' modundaysa ve belirli aralÄ±klarla # "ansi" -> "fast"
                    self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            rewards_per_episode.append(total_reward) # BÃ¶lÃ¼m Ã¶dÃ¼lÃ¼nÃ¼ listeye ekle.
            steps_per_episode.append(self.env.steps) # BÃ¶lÃ¼m adÄ±m sayÄ±sÄ±nÄ± listeye ekle.
            self.agent.decay_epsilon() # Epsilon deÄŸerini azalt.
            self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            self.progress.emit(episode+1, total_reward, self.env.steps, self.agent.epsilon) # Ä°lerleme sinyalini gÃ¶nder.
        self.finished.emit(rewards_per_episode, steps_per_episode) # EÄŸitim bitti sinyalini gÃ¶nder.
    def stop(self):
        # EÄŸitimi durdurmak iÃ§in kullanÄ±lÄ±r.
        self.running = False

# =====================
# Grid ve Bilgi Paneli (PyQt5)
# =====================
class GridWidget(QWidget): # OrtamÄ±n grid yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtiren widget.
    def __init__(self, env, parent=None):
        super().__init__(parent)
        self.env = env # GÃ¶rselleÅŸtirilecek ortam.
        self.cell_size = 80 # Her bir grid hÃ¼cresinin piksel boyutu.
        self.setMinimumSize(env.grid_size * self.cell_size, env.grid_size * self.cell_size)
        # Renkler ve gÃ¶rsel ayarlar
        self.colors = {
            'background': Qt.white,
            'grid': Qt.lightGray,
            'drone': Qt.blue,
            'drone_landed': QColor(100, 100, 180), # Ä°niÅŸ yapmÄ±ÅŸ drone rengi.
            'cargo_depot': Qt.green, # Kargo deposu rengi.
            'delivery_point': Qt.red, # Teslimat noktasÄ± rengi.
            'cargo': Qt.green, # Kargo rengi.
            'shadow': QColor(100, 100, 100, 80) # Drone uÃ§arkenki gÃ¶lge rengi.
        }
    def paintEvent(self, event):
        # Grid ve tÃ¼m nesneleri Ã§iz
        # Bu fonksiyon, widget her yeniden Ã§izildiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing) # Daha pÃ¼rÃ¼zsÃ¼z Ã§izimler iÃ§in.
        painter.fillRect(self.rect(), self.colors['background']) # Arka planÄ± boya.
        # Ortalamak iÃ§in offset hesapla
        grid_pixel_size = self.env.grid_size * self.cell_size
        x_offset = (self.width() - grid_pixel_size) // 2
        y_offset = (self.height() - grid_pixel_size) // 2
        # Grid Ã§izgileri
        painter.setPen(QPen(self.colors['grid'], 1))
        for i in range(self.env.grid_size + 1):
            painter.drawLine(x_offset, y_offset + i * self.cell_size, x_offset + self.env.grid_size * self.cell_size, y_offset + i * self.cell_size)
            painter.drawLine(x_offset + i * self.cell_size, y_offset, x_offset + i * self.cell_size, y_offset + self.env.grid_size * self.cell_size)
        # Kargo deposu Ã§izimi
        depot_x = x_offset + self.env.cargo_depot_pos[1] * self.cell_size + self.cell_size // 2
        depot_y = y_offset + self.env.cargo_depot_pos[0] * self.cell_size + self.cell_size // 2
        painter.setBrush(QBrush(self.colors['cargo_depot']))
        painter.setPen(Qt.NoPen) # Kenar Ã§izgisi olmasÄ±n.
        painter.drawEllipse(depot_x - self.cell_size // 3, depot_y - self.cell_size // 3, 2 * self.cell_size // 3, 2 * self.cell_size // 3)
        # Teslimat noktalarÄ± Ã§izimi
        painter.setBrush(QBrush(self.colors['delivery_point']))
        for i, point in enumerate(self.env.delivery_points):
            if i < len(self.env.delivered) and not self.env.delivered[i]: # HenÃ¼z teslim edilmemiÅŸse Ã§iz.
                x = x_offset + point[1] * self.cell_size + self.cell_size // 2
                y = y_offset + point[0] * self.cell_size + self.cell_size // 2
                painter.drawEllipse(x - self.cell_size // 4, y - self.cell_size // 4, self.cell_size // 2, self.cell_size // 2)
                painter.setPen(Qt.black) # Teslimat noktasÄ± numarasÄ±nÄ± yazmak iÃ§in.
                painter.setFont(QFont('Arial', 10))
                painter.drawText(x - 5, y + 5, str(i + 1)) # Teslimat noktasÄ± numarasÄ±nÄ± yaz.
                painter.setPen(Qt.NoPen)
        # Drone Ã§izimi
        drone_x = x_offset + self.env.drone_pos[1] * self.cell_size + self.cell_size // 2
        drone_y = y_offset + self.env.drone_pos[0] * self.cell_size + self.cell_size // 2
        if self.env.is_flying: # Drone uÃ§uyorsa
            # GÃ¶lge efekti
            painter.setBrush(QBrush(self.colors['shadow']))
            painter.drawEllipse(drone_x - self.cell_size // 6, drone_y + self.cell_size // 4, self.cell_size // 3, self.cell_size // 8)
            height_offset = 0 # YÃ¼kseklik ofseti (animasyon iÃ§in).
            if self.env.landing_state == "taking_off": # KalkÄ±ÅŸ animasyonu
                height_offset = -5 * self.env.landing_animation_step
            elif self.env.landing_state == "landing": # Ä°niÅŸ animasyonu
                height_offset = -15 + 5 * self.env.landing_animation_step
            elif self.env.landing_state == "flying": # Normal uÃ§uÅŸ
                height_offset = -15
            drone_y += height_offset # Drone'un dikey konumunu ayarla.
            painter.setBrush(QBrush(self.colors['drone'])) # UÃ§an drone rengi.
        else: # Drone yerdeyse
            painter.setBrush(QBrush(self.colors['drone_landed'])) # Ä°niÅŸ yapmÄ±ÅŸ drone rengi.
        # Drone gÃ¶vdesi
        painter.drawEllipse(drone_x - self.cell_size // 4, drone_y - self.cell_size // 4, self.cell_size // 2, self.cell_size // 2)
        # Pervaneler
        propeller_size = self.cell_size // 8
        if self.env.is_flying: # UÃ§arken pervaneler daha bÃ¼yÃ¼k gÃ¶rÃ¼nebilir.
            propeller_size = self.cell_size // 6
        painter.setBrush(QBrush(Qt.black)) # Pervane rengi.
        # Sol Ã¼st
        painter.drawEllipse(drone_x - propeller_size - propeller_size//2, drone_y - propeller_size - propeller_size//2, propeller_size, propeller_size)
        # SaÄŸ Ã¼st
        painter.drawEllipse(drone_x + propeller_size - propeller_size//2, drone_y - propeller_size - propeller_size//2, propeller_size, propeller_size)
        # Sol alt
        painter.drawEllipse(drone_x - propeller_size - propeller_size//2, drone_y + propeller_size - propeller_size//2, propeller_size, propeller_size)
        # SaÄŸ alt
        painter.drawEllipse(drone_x + propeller_size - propeller_size//2, drone_y + propeller_size - propeller_size//2, propeller_size, propeller_size)
        # Kargo Ã§izimi
        if self.env.has_cargo: # EÄŸer drone kargo taÅŸÄ±yorsa
            painter.setBrush(QBrush(self.colors['cargo'])) # Kargo rengi.
            painter.drawRect(drone_x - self.cell_size // 8, drone_y - self.cell_size // 8, self.cell_size // 4, self.cell_size // 4)
        # Batarya gÃ¶stergesi
        painter.setPen(Qt.black)
        painter.setFont(QFont('Arial', 10))
        painter.drawText(drone_x - 20, drone_y - 30, f"ğŸ”‹: {self.env.battery}%") # Drone Ã¼zerinde batarya seviyesini gÃ¶ster.

class InfoPanelWidget(QWidget): # Ortam ve eÄŸitim bilgilerini gÃ¶steren widget.
    def __init__(self, env, parent=None):
        super().__init__(parent)
        self.env = env # Bilgileri gÃ¶sterilecek ortam.
        layout = QVBoxLayout()
        # Bilgi paneli iÃ§in GroupBox
        info_group = QGroupBox("â„¹ï¸ Durum Bilgileri")
        info_layout = QVBoxLayout()
        self.battery_label = QLabel() # Batarya bilgisi etiketi.
        self.cargo_label = QLabel() # Kargo durumu etiketi.
        self.delivery_label = QLabel() # Teslimat durumu etiketi.
        self.steps_label = QLabel() # AdÄ±m sayÄ±sÄ± etiketi.
        self.reward_label = QLabel()  # Son Ã¶dÃ¼l etiketi
        self.total_reward_label = QLabel()  # Toplam Ã¶dÃ¼l etiketi
        self.last_action_label = QLabel()  # Son aksiyon etiketi
        self.training_progress_label = QLabel()  # EÄŸitim ilerlemesi etiketi
        info_layout.addWidget(self.battery_label)
        info_layout.addWidget(self.cargo_label)
        info_layout.addWidget(self.delivery_label)
        info_layout.addWidget(self.steps_label)
        info_layout.addWidget(self.reward_label)  # Son Ã¶dÃ¼l panelde gÃ¶ster
        info_layout.addWidget(self.total_reward_label)  # Toplam Ã¶dÃ¼l panelde gÃ¶ster
        info_layout.addWidget(self.last_action_label)  # Son aksiyon panelde gÃ¶ster
        info_layout.addWidget(self.training_progress_label)  # Durum bilgisine eklendi
        info_group.setLayout(info_layout)
        self.status_label = QLabel() # Genel durum mesajlarÄ± iÃ§in etiket.
        # Ana layout
        layout.addWidget(info_group)
        layout.addWidget(self.status_label)
        layout.addStretch() # Widget'larÄ± yukarÄ±ya iter.
        self.setLayout(layout)
        self.update_info() # Bilgileri ilk kez gÃ¼ncelle.
    def update_info(self):
        # Paneldeki tÃ¼m bilgileri gÃ¼nceller.
        self.battery_label.setText(f"ğŸ”‹ Batarya: %{self.env.battery}")
        # Kargo etiketi: taÅŸÄ±nÄ±yorsa kalÄ±n yeÅŸil
        if self.env.has_cargo:
            self.cargo_label.setText("ğŸ“¦ Kargo: <span style='color:#1ca81c; font-weight:bold;'>TaÅŸÄ±nÄ±yor</span>")
            self.cargo_label.setTextFormat(Qt.RichText) # HTML formatÄ±nda metin.
            self.cargo_label.setStyleSheet("")
        else:
            self.cargo_label.setText("ğŸ“¦ Kargo: Yok")
            self.cargo_label.setTextFormat(Qt.AutoText)
            self.cargo_label.setStyleSheet("")
        # Teslimat etiketi: teslim edilen sayÄ± yeÅŸil ve kalÄ±n
        delivered_count = sum(self.env.delivered) # Teslim edilen paket sayÄ±sÄ±.
        total = len(self.env.delivery_points) # Toplam teslimat noktasÄ± sayÄ±sÄ±.
        if delivered_count > 0:
            self.delivery_label.setText(f"ğŸ¯ Teslimatlar: <span style='color:#1ca81c; font-weight:bold;'>{delivered_count}</span>/{total}")
            self.delivery_label.setTextFormat(Qt.RichText)
        else:
            self.delivery_label.setText(f"ğŸ¯ Teslimatlar: 0/{total}")
            self.delivery_label.setTextFormat(Qt.AutoText)
        self.steps_label.setText(f"ğŸ‘£ AdÄ±m: {self.env.steps}")
        self.reward_label.setText(f"ğŸ… Son Ã–dÃ¼l: {self.env.last_reward:.2f}")  # Son Ã¶dÃ¼l gÃ¶sterimi
        self.total_reward_label.setText(f"ğŸ¥‡ Toplam Ã–dÃ¼l: {self.env.total_reward:.2f}")  # Toplam Ã¶dÃ¼l gÃ¶sterimi
        # Son aksiyon bilgisini grup kutusunda gÃ¶ster
        if hasattr(self.env, 'last_action_info') and self.env.last_action_info:
            self.last_action_label.setText(f"ğŸ”„ Son Aksiyon: {self.env.last_action_info}")
        else:
            self.last_action_label.setText("ğŸ”„ Son Aksiyon: -")
        # EÄŸitim ilerlemesi sadece eÄŸitim sÄ±rasÄ±nda gÃ¶sterilecek, aksi halde gizle
        if not self.training_progress_label.text(): # EÄŸer eÄŸitim ilerleme metni boÅŸsa
            self.training_progress_label.setVisible(False) # Etiketi gizle.
        else:
            self.training_progress_label.setVisible(True) # Etiketi gÃ¶ster.
    def set_status(self, status):
        # Genel durum mesajÄ±nÄ± ayarlar.
        self.status_label.setText(status)
    def set_training_progress(self, episode, total_episodes, reward, steps, epsilon):
        # EÄŸitim ilerleme bilgisini ayarlar.
        self.training_progress_label.setText(f"ğŸ“ˆ Episode: {episode}/{total_episodes} | Ã–dÃ¼l: {reward:.2f} | AdÄ±m: {steps}")
        self.training_progress_label.setVisible(True) # Etiketi gÃ¶rÃ¼nÃ¼r yap.
    def clear_training_progress(self):
        # EÄŸitim ilerleme bilgisini temizler ve gizler.
        self.training_progress_label.setText("")
        self.training_progress_label.setVisible(False)

# =====================
# Neural Network GÃ¶rselleÅŸtirme Widget'Ä±
# =====================
class NeuralNetworkWidget(QWidget):
    """
    Basit ve anlaÅŸÄ±lÄ±r DQN neural network gÃ¶rselleÅŸtirmesi
    - 3 katman: Input â†’ Hidden â†’ Output
    - Her katmanda 5-6 nÃ¶ron
    - Renkli aktivasyon gÃ¶sterimi
    """
    def __init__(self, agent, parent=None):
        super().__init__(parent)
        self.agent = agent
        self.setMinimumSize(300, 200)
        self.setMaximumSize(300, 220)
        
        # Basit katman konfigÃ¼rasyonu
        self.layer_names = ["Input", "Hidden", "Output"]
        self.layer_sizes = [5, 6, 6]  # GÃ¶rselleÅŸtirme iÃ§in basit
        self.action_names = ["â†“", "â†’", "â†‘", "â†", "ğŸ“¦", "ğŸ›«"]
        
        # Aktivasyon verileri
        self.activations = [np.random.random(size) * 0.3 for size in self.layer_sizes]
        self.last_action = 0
        
        # Basit renkler
        self.bg_color = QColor(240, 240, 240)
        self.neuron_colors = {
            'inactive': QColor(200, 200, 200),
            'low': QColor(150, 200, 255),
            'medium': QColor(100, 150, 255),
            'high': QColor(255, 100, 100),
            'selected': QColor(50, 255, 50)
        }
        self.text_color = Qt.black
        self.connection_color = QColor(150, 150, 150, 100)
        
    def update_activations(self, state, q_values=None, selected_action=None):
        """Agent'tan gelen state ve Q-values ile aktivasyonlarÄ± gÃ¼ncelle"""
        try:
            if isinstance(state, np.ndarray) and len(state) >= 5:
                # Input layer - state'in Ã¶nemli parÃ§alarÄ±nÄ± seÃ§
                self.activations[0] = np.array([
                    state[0] if len(state) > 0 else 0,  # drone_x
                    state[1] if len(state) > 1 else 0,  # drone_y
                    state[2] if len(state) > 2 else 0,  # has_cargo
                    state[3] if len(state) > 3 else 0,  # is_flying
                    state[4] if len(state) > 4 else 0   # battery
                ])
                
                # Hidden layer - simulated values
                input_avg = np.mean(self.activations[0])
                self.activations[1] = np.random.normal(input_avg, 0.2, 6)
                self.activations[1] = np.clip(self.activations[1], 0, 1)
            
            if q_values is not None and len(q_values) >= 6:
                # Output layer - Q-values normalize
                q_min, q_max = np.min(q_values), np.max(q_values)
                if q_max > q_min:
                    self.activations[2] = (q_values - q_min) / (q_max - q_min)
                else:
                    self.activations[2] = np.ones(6) * 0.5
                    
            if selected_action is not None:
                self.last_action = selected_action
                
        except Exception as e:
            # Fallback random values
            for i in range(len(self.activations)):
                self.activations[i] = np.random.random(self.layer_sizes[i]) * 0.5
        
        self.update()  # Widget'Ä± yeniden Ã§iz
    
    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        painter.fillRect(self.rect(), self.bg_color)
        
        width = self.width()
        height = self.height()
        
        # Title
        painter.setPen(self.text_color)
        painter.setFont(QFont('Arial', 12, QFont.Bold))
        painter.drawText(10, 20, "ğŸ§  Neural Network")
        
        # Katman pozisyonlarÄ±
        layer_x = [80, 150, 220]  # 3 katman iÃ§in sabit pozisyonlar
        start_y = 50
        layer_height = height - 100
        
        # BaÄŸlantÄ±larÄ± Ã¶nce Ã§iz (nÃ¶ronlarÄ±n altÄ±nda kalmasÄ± iÃ§in)
        for layer_idx in range(len(self.layer_sizes) - 1):
            x1 = layer_x[layer_idx]
            x2 = layer_x[layer_idx + 1]
            
            for i in range(self.layer_sizes[layer_idx]):
                y1 = start_y + (i + 1) * layer_height / (self.layer_sizes[layer_idx] + 1)
                
                for j in range(self.layer_sizes[layer_idx + 1]):
                    y2 = start_y + (j + 1) * layer_height / (self.layer_sizes[layer_idx + 1] + 1)
                    
                    # BaÄŸlantÄ± rengi aktivasyona gÃ¶re
                    activation = self.activations[layer_idx][i] if i < len(self.activations[layer_idx]) else 0
                    conn_color = QColor(self.connection_color)
                    conn_color.setAlpha(int(50 + activation * 100))
                    
                    painter.setPen(QPen(conn_color, 1))
                    painter.drawLine(int(x1 + 8), int(y1), int(x2 - 8), int(y2))
        
        # NÃ¶ronlarÄ± Ã§iz
        for layer_idx, layer_size in enumerate(self.layer_sizes):
            x = layer_x[layer_idx]
            
            # Katman baÅŸlÄ±ÄŸÄ±
            painter.setPen(self.text_color)
            painter.setFont(QFont('Arial', 10, QFont.Bold))
            painter.drawText(int(x - 20), 40, self.layer_names[layer_idx])
            
            for neuron_idx in range(layer_size):
                y = start_y + (neuron_idx + 1) * layer_height / (layer_size + 1)
                
                # Aktivasyon deÄŸeri
                activation = self.activations[layer_idx][neuron_idx] if neuron_idx < len(self.activations[layer_idx]) else 0
                
                # NÃ¶ron rengi
                if layer_idx == 2 and neuron_idx == self.last_action:  # SeÃ§ilen eylem
                    color = self.neuron_colors['selected']
                elif activation > 0.7:
                    color = self.neuron_colors['high']
                elif activation > 0.4:
                    color = self.neuron_colors['medium']
                elif activation > 0.1:
                    color = self.neuron_colors['low']
                else:
                    color = self.neuron_colors['inactive']
                
                # NÃ¶ron Ã§iz
                painter.setBrush(QBrush(color))
                painter.setPen(QPen(Qt.black, 1))
                painter.drawEllipse(int(x - 8), int(y - 8), 16, 16)
                
                # Output layer iÃ§in eylem etiketleri
                if layer_idx == 2 and neuron_idx < len(self.action_names):
                    painter.setPen(self.text_color)
                    painter.setFont(QFont('Arial', 8))
                    painter.drawText(int(x + 12), int(y + 4), self.action_names[neuron_idx])
                
                # Aktivasyon deÄŸerini gÃ¶ster
                if activation > 0.1:
                    painter.setPen(Qt.white)
                    painter.setFont(QFont('Arial', 6))
                    painter.drawText(int(x - 4), int(y + 2), f"{activation:.1f}")
        
        # Legend
        painter.setPen(self.text_color)
        painter.setFont(QFont('Arial', 8))
        painter.drawText(10, height - 15, "ğŸ”´ YÃ¼ksek  ğŸ”µ Orta  âšª DÃ¼ÅŸÃ¼k")

# =====================
# Ana PyQt5 ArayÃ¼zÃ¼
# =====================
class DroneDeliverySimulator(QMainWindow): # Ana uygulama penceresi.
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Paket DaÄŸÄ±tÄ±m DronlarÄ± SimÃ¼latÃ¶rÃ¼ - DQN") # Pencere baÅŸlÄ±ÄŸÄ±.

        # Emoji ikonu oluÅŸturma
        emoji = "ğŸš"
        pixmap = QPixmap(64, 64) # Ä°kon boyutu
        pixmap.fill(Qt.transparent) # Åeffaf arka plan
        painter = QPainter(pixmap)
        font = QFont()
        font.setPointSize(48) # Emoji boyutu
        painter.setFont(font)
        painter.drawText(pixmap.rect(), Qt.AlignCenter, emoji)
        painter.end()
        self.setWindowIcon(QIcon(pixmap))

        self.resize(1200, 700) # Pencere boyutu.
        self.grid_size = 5 # BaÅŸlangÄ±Ã§ grid boyutu.
        self.env = DroneDeliveryEnv(grid_size=self.grid_size) # OrtamÄ± oluÅŸtur.
        self.agent = DQNAgent(self.env) # DQN ajanÄ± oluÅŸtur.
        self.training_thread = None # EÄŸitim thread'i baÅŸlangÄ±Ã§ta yok.
        self.sim_speed = 50  # AI ile oyna hÄ±z (ms).
        # --- Ana Layout ---
        central_widget = QWidget()
        self.setCentralWidget(central_widget) # Ana widget'Ä± ayarla.
        main_layout = QHBoxLayout(central_widget) # Ana layout (yatay).
        # --- Sol Panel: Parametreler ve Kontroller ---
        left_panel = QWidget()
        left_layout = QVBoxLayout(left_panel) # Sol panel layout'u (dikey).
        # Grid boyutu
        grid_group = QGroupBox("ğŸ—ºï¸ Grid AyarlarÄ±")
        grid_layout = QHBoxLayout()
        grid_layout.addWidget(QLabel("Grid Boyutu:"))
        self.grid_size_spin = QSpinBox() # Grid boyutu iÃ§in spin box.
        self.grid_size_spin.setRange(3, 7) # Min ve max grid boyutu.
        self.grid_size_spin.setValue(self.grid_size)
        self.grid_size_spin.valueChanged.connect(self.update_grid_size) # DeÄŸer deÄŸiÅŸtiÄŸinde fonksiyon Ã§aÄŸÄ±r.
        grid_layout.addWidget(self.grid_size_spin)
        grid_group.setLayout(grid_layout)
        left_layout.addWidget(grid_group)
        
        # DQN parametreleri
        dqn_group = QGroupBox("ğŸ¤– DQN Parametreleri")
        dqn_layout = QGridLayout() # Parametreleri grid iÃ§inde dÃ¼zenle.
        dqn_layout.addWidget(QLabel("Learning Rate:"), 0, 0)
        self.lr_spin = QDoubleSpinBox(); self.lr_spin.setRange(0.0001, 0.01); self.lr_spin.setSingleStep(0.0001); self.lr_spin.setValue(0.001); self.lr_spin.setDecimals(4)
        dqn_layout.addWidget(self.lr_spin, 0, 1)
        dqn_layout.addWidget(QLabel("Gamma:"), 1, 0)
        self.gamma_spin = QDoubleSpinBox(); self.gamma_spin.setRange(0.1, 0.999); self.gamma_spin.setSingleStep(0.01); self.gamma_spin.setValue(0.99)
        dqn_layout.addWidget(self.gamma_spin, 1, 1)
        dqn_layout.addWidget(QLabel("Epsilon (KeÅŸif OranÄ±):"), 2, 0)
        self.epsilon_spin = QDoubleSpinBox(); self.epsilon_spin.setRange(0.1, 1.0); self.epsilon_spin.setSingleStep(0.1); self.epsilon_spin.setValue(1.0)
        dqn_layout.addWidget(self.epsilon_spin, 2, 1)
        dqn_layout.addWidget(QLabel("Epsilon Decay:"), 3, 0)
        self.epsilon_decay_spin = QDoubleSpinBox(); self.epsilon_decay_spin.setRange(0.9, 0.9999); self.epsilon_decay_spin.setSingleStep(0.001); self.epsilon_decay_spin.setValue(0.998)
        dqn_layout.addWidget(self.epsilon_decay_spin, 3, 1)
        dqn_layout.addWidget(QLabel("Min Epsilon:"), 4, 0)
        self.min_epsilon_spin = QDoubleSpinBox(); self.min_epsilon_spin.setRange(0.01, 0.5); self.min_epsilon_spin.setSingleStep(0.01); self.min_epsilon_spin.setValue(0.01)
        dqn_layout.addWidget(self.min_epsilon_spin, 4, 1)
        dqn_layout.addWidget(QLabel("Batch Size:"), 5, 0)
        self.batch_size_spin = QSpinBox(); self.batch_size_spin.setRange(16, 128); self.batch_size_spin.setSingleStep(16); self.batch_size_spin.setValue(32)
        dqn_layout.addWidget(self.batch_size_spin, 5, 1)
        dqn_layout.addWidget(QLabel("EÄŸitim Episodes:"), 6, 0)
        self.episodes_spin = QSpinBox(); self.episodes_spin.setRange(1000, 100000); self.episodes_spin.setSingleStep(1000); self.episodes_spin.setValue(20000) # 20bin episode default
        dqn_layout.addWidget(self.episodes_spin, 6, 1)
        dqn_group.setLayout(dqn_layout)
        left_layout.addWidget(dqn_group)
        # EÄŸitim hÄ±zÄ± (mod ve delay)
        speed_group = QGroupBox("âš¡ EÄŸitim/SimÃ¼lasyon HÄ±zÄ±")
        speed_layout = QGridLayout()
        speed_layout.addWidget(QLabel("EÄŸitim Modu:"), 0, 0)
        self.training_mode_combo = QComboBox(); 
        self.training_mode_combo.addItems(["Fast Mode", "Human Mode"]) # EÄŸitim modu seÃ§enekleri.
        speed_layout.addWidget(self.training_mode_combo, 0, 1)
        speed_layout.addWidget(QLabel("EÄŸitim HÄ±zÄ± (ms):"), 1, 0) # CanlÄ± mod iÃ§in eÄŸitim hÄ±zÄ±.
        speed_slider_row = QHBoxLayout()
        speed_slider_row.addWidget(QLabel("ğŸï¸"))
        self.training_speed_slider = QSlider(Qt.Horizontal); self.training_speed_slider.setRange(10, 1000); self.training_speed_slider.setValue(100) # HÄ±z ayarÄ± iÃ§in slider.
        speed_slider_row.addWidget(self.training_speed_slider)
        speed_slider_row.addWidget(QLabel("ğŸ¢"))
        speed_layout.addLayout(speed_slider_row, 1, 1)
        speed_layout.addWidget(QLabel("SimÃ¼lasyon HÄ±zÄ± (ms):"), 2, 0) # AI ile oynama hÄ±zÄ±.
        sim_slider_row = QHBoxLayout()
        sim_slider_row.addWidget(QLabel("ğŸï¸"))
        self.sim_speed_slider = QSlider(Qt.Horizontal); self.sim_speed_slider.setRange(10, 1000); self.sim_speed_slider.setValue(self.sim_speed)
        self.sim_speed_slider.valueChanged.connect(self.update_sim_speed) # DeÄŸer deÄŸiÅŸtiÄŸinde fonksiyon Ã§aÄŸÄ±r.
        sim_slider_row.addWidget(self.sim_speed_slider)
        sim_slider_row.addWidget(QLabel("ğŸ¢"))
        speed_layout.addLayout(sim_slider_row, 2, 1)
        speed_group.setLayout(speed_layout)
        left_layout.addWidget(speed_group)
        # EÄŸitim kontrolleri
        training_group = QGroupBox("ğŸ“ EÄŸitim")
        training_layout = QVBoxLayout()
        self.train_button = QPushButton("ğŸš€ EÄŸitimi BaÅŸlat"); self.train_button.clicked.connect(self.start_training) # EÄŸitimi baÅŸlat butonu.
        self.stop_button = QPushButton("â¹ï¸ EÄŸitimi Durdur"); self.stop_button.clicked.connect(self.stop_training); self.stop_button.setEnabled(False) # EÄŸitimi durdur butonu (baÅŸlangÄ±Ã§ta pasif).
        self.save_button = QPushButton("ğŸ’¾ Modeli Kaydet"); self.save_button.clicked.connect(self.save_model); self.save_button.setEnabled(False) # Modeli kaydet butonu (baÅŸlangÄ±Ã§ta pasif).
        self.load_button = QPushButton("ğŸ“‚ Modeli YÃ¼kle"); self.load_button.clicked.connect(self.load_model) # Modeli yÃ¼kle butonu.
        training_layout.addWidget(self.train_button)
        training_layout.addWidget(self.stop_button)
        training_layout.addWidget(self.save_button)
        training_layout.addWidget(self.load_button)
        training_group.setLayout(training_layout)
        left_layout.addWidget(training_group)
        # Oyun kontrolleri
        game_group = QGroupBox("ğŸ® Oyun Kontrolleri")
        game_layout = QVBoxLayout()
        self.ai_button = QPushButton("ğŸ¤– AI ile Oyna"); self.ai_button.clicked.connect(self.play_with_ai); self.ai_button.setEnabled(False) # AI ile oyna butonu (baÅŸlangÄ±Ã§ta pasif).
        self.human_button = QPushButton("ğŸ§‘â€ğŸ’» Ä°nsan Modu (Manuel Oyna)"); self.human_button.clicked.connect(self.play_human_mode) # Manuel oynama butonu.
        self.stop_game_button = QPushButton("â¹ï¸ Oyunu Durdur"); self.stop_game_button.clicked.connect(self.stop_game); self.stop_game_button.setEnabled(False) # Oyunu durdur butonu (baÅŸlangÄ±Ã§ta pasif).
        self.reset_button = QPushButton("ğŸ”„ SÄ±fÄ±rla"); self.reset_button.clicked.connect(self.reset_env) # OrtamÄ± sÄ±fÄ±rla butonu.
        game_layout.addWidget(self.ai_button)
        game_layout.addWidget(self.human_button)
        game_layout.addWidget(self.stop_game_button)
        game_layout.addWidget(self.reset_button)
        game_group.setLayout(game_layout)
        left_layout.addWidget(game_group)
        
        left_layout.addStretch() # Sol paneli yukarÄ± iter.        # --- SaÄŸ Panel: Grid ve Neural Network ---
        right_panel = QWidget()
        right_layout = QHBoxLayout(right_panel) # SaÄŸ panel layout'u (yatay).
        
        # Grid bÃ¶lÃ¼mÃ¼
        grid_section = QWidget()
        grid_section_layout = QVBoxLayout(grid_section)
        self.grid_widget = GridWidget(self.env) # Grid widget'Ä±nÄ± oluÅŸtur.
        self.info_panel = InfoPanelWidget(self.env) # Bilgi paneli widget'Ä±nÄ± oluÅŸtur.
        grid_section_layout.addWidget(self.grid_widget, 7) # Grid widget'Ä±nÄ± ekle (daha fazla yer kaplasÄ±n).
        grid_section_layout.addWidget(self.info_panel, 3) # Bilgi panelini ekle.
        
        # Neural Network bÃ¶lÃ¼mÃ¼
        nn_section = QWidget()
        nn_section_layout = QVBoxLayout(nn_section)
        nn_group = QGroupBox("ğŸ§  Neural Network")
        nn_group_layout = QVBoxLayout()
        self.nn_widget = NeuralNetworkWidget(self.agent)
        nn_group_layout.addWidget(self.nn_widget)
        nn_group.setLayout(nn_group_layout)
        nn_section_layout.addWidget(nn_group)
        nn_section_layout.addStretch() # Neural network bÃ¶lÃ¼mÃ¼nÃ¼ yukarÄ± iter.
        
        # SaÄŸ panele bÃ¶lÃ¼mleri ekle
        right_layout.addWidget(grid_section, 3) # Grid bÃ¶lÃ¼mÃ¼ (daha fazla yer)
        right_layout.addWidget(nn_section, 1) # Neural network bÃ¶lÃ¼mÃ¼ (daha az yer)
        # --- LayoutlarÄ± birleÅŸtir ---
        main_layout.addWidget(left_panel, 1) # Sol paneli ana layout'a ekle (daha az yer kaplasÄ±n).
        main_layout.addWidget(right_panel, 4) # SaÄŸ paneli ana layout'a ekle (daha fazla yer kaplasÄ±n).
        # --- Timer ---
        self.game_timer = QTimer(); self.game_timer.timeout.connect(self.update_game) # Oyun dÃ¶ngÃ¼sÃ¼ iÃ§in timer.
        self.game_mode = None # Oyun modu (ai, human, None).
        self.model_trained = False # Modelin eÄŸitilip eÄŸitilmediÄŸi.
        self.model_loaded = False # Modelin yÃ¼klenip yÃ¼klenmediÄŸi.
        self.update_ui() # ArayÃ¼zÃ¼ ilk kez gÃ¼ncelle.
        self.statusBar().showMessage("HazÄ±r - EÄŸitim veya AI ile oynamak iÃ§in modeli eÄŸitin/yÃ¼kleyin.") # Durum Ã§ubuÄŸu mesajÄ±.
        
        # GitHub linki
        github_link_label = QLabel('Coded by <a href="https://github.com/FerhatAkalan">Ferhat Akalan</a>')
        github_link_label.setOpenExternalLinks(True)
        self.statusBar().addPermanentWidget(github_link_label)

    def set_params_enabled(self, enabled: bool):
        # DQN parametrelerinin aktif/pasif durumunu ayarlar.
        self.grid_size_spin.setEnabled(enabled)
        self.lr_spin.setEnabled(enabled)
        self.gamma_spin.setEnabled(enabled)
        self.epsilon_spin.setEnabled(enabled)
        self.epsilon_decay_spin.setEnabled(enabled)
        self.min_epsilon_spin.setEnabled(enabled)
        self.batch_size_spin.setEnabled(enabled)
        self.episodes_spin.setEnabled(enabled)
        self.training_mode_combo.setEnabled(enabled)
        self.training_speed_slider.setEnabled(enabled)
        self.sim_speed_slider.setEnabled(enabled)

    def set_game_buttons_enabled(self, enabled: bool):
        # Oyunla ilgili butonlarÄ±n aktif/pasif durumunu ayarlar.
        self.train_button.setEnabled(enabled)
        self.ai_button.setEnabled(enabled and (self.model_trained or self.model_loaded)) # AI butonu model varsa aktif olur.
        self.human_button.setEnabled(enabled)
        self.reset_button.setEnabled(enabled)
        self.save_button.setEnabled(enabled and self.model_trained) # Kaydet butonu model eÄŸitildiyse aktif olur.
        self.load_button.setEnabled(enabled)

    def update_grid_size(self):
        # Grid boyutu deÄŸiÅŸtiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        self.grid_size = self.grid_size_spin.value()
        self.reset_env() # OrtamÄ± yeni grid boyutuyla sÄ±fÄ±rla.
    def update_sim_speed(self):
        # SimÃ¼lasyon hÄ±zÄ± (AI ile oynama hÄ±zÄ±) deÄŸiÅŸtiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        self.sim_speed = self.sim_speed_slider.value()
        if self.game_timer.isActive(): # EÄŸer oyun zamanlayÄ±cÄ±sÄ± aktifse
            self.game_timer.setInterval(self.sim_speed) # ZamanlayÄ±cÄ±nÄ±n aralÄ±ÄŸÄ±nÄ± gÃ¼ncelle.
    def reset_env(self):
        # OrtamÄ± ve ajanÄ± sÄ±fÄ±rlar.
        if self.game_timer.isActive(): # EÄŸer oyun zamanlayÄ±cÄ±sÄ± aktifse durdur.
            self.game_timer.stop(); self.game_mode = None
        self.env = DroneDeliveryEnv(grid_size=self.grid_size) # Yeni ortam oluÅŸtur.
        self.agent = DQNAgent(self.env) # Yeni DQN ajanÄ± oluÅŸtur.
        self.grid_widget.env = self.env # Grid widget'Ä±nÄ±n ortamÄ±nÄ± gÃ¼ncelle.
        self.info_panel.env = self.env # Bilgi panelinin ortamÄ±nÄ± gÃ¼ncelle.
        self.model_trained = False # Model eÄŸitilmedi olarak iÅŸaretle.
        self.model_loaded = False # Model yÃ¼klenmedi olarak iÅŸaretle.
        self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
        self.set_game_buttons_enabled(True) # ButonlarÄ± aktif et.
        self.set_params_enabled(True) # Parametreleri aktif et.
        self.info_panel.clear_training_progress() # EÄŸitim ilerlemesini temizle.
        self.statusBar().showMessage("Ortam sÄ±fÄ±rlandÄ±")

    def update_ui(self):
        # ArayÃ¼zdeki grid ve bilgi panelini gÃ¼nceller.
        self.grid_widget.update(); self.info_panel.update_info()
        
        # Neural network gÃ¶rselleÅŸtirmesini gÃ¼ncelle
        if hasattr(self, 'nn_widget'):
            current_state = self.env.get_state()
            # Agent'tan Q-values al
            with torch.no_grad():
                state_tensor = torch.FloatTensor(current_state).unsqueeze(0)
                q_values = self.agent.q_network(state_tensor).squeeze().numpy()
                selected_action = self.agent.select_action(current_state, training=False)
            self.nn_widget.update_activations(current_state, q_values, selected_action)

    def update_game(self):
        # AI ile oynama modunda oyunun bir adÄ±mÄ±nÄ± gÃ¼nceller.
        if self.game_mode == 'ai':
            if not hasattr(self, 'ai_episode_count'):
                self.ai_episode_count = 1
                self.ai_total_reward = 0
            
            state = self.env.get_state()
            action = self.agent.select_action(state, training=False)
            next_state, reward, done, info = self.env.step(action)
            self.ai_total_reward += reward
            self.update_ui()
            if done:
                delivered = sum(self.env.delivered)
                # Bilgi panelinde ve durum Ã§ubuÄŸunda bÃ¶lÃ¼m sonucunu gÃ¶ster.
                self.info_panel.set_status(f"ğŸ¤–AI Episode: {self.ai_episode_count} | ğŸ¯Teslimat: {delivered}/{len(self.env.delivery_points)} | ğŸ”‹Batarya: %{self.env.battery} | ğŸ¥‡Skor: {self.ai_total_reward:.2f}")
                self.ai_episode_count += 1
                self.ai_total_reward = 0
                # Episode bittiÄŸinde otomatik olarak yeni episode baÅŸlat
                self.env.reset()
                self.update_ui()
    def start_training(self):
        # EÄŸitimi baÅŸlatÄ±r.
        # Ajan parametrelerini arayÃ¼zdeki deÄŸerlerle gÃ¼nceller.
        self.agent.lr = self.lr_spin.value()
        self.agent.gamma = self.gamma_spin.value()
        self.agent.epsilon = self.epsilon_spin.value()
        self.agent.epsilon_decay = self.epsilon_decay_spin.value()
        self.agent.min_epsilon = self.min_epsilon_spin.value()
        self.agent.batch_size = self.batch_size_spin.value()
        # Optimizer'Ä± yeni learning rate ile gÃ¼ncelle
        self.agent.optimizer = optim.Adam(self.agent.q_network.parameters(), lr=self.agent.lr)
        episodes = self.episodes_spin.value() # EÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ±nÄ± al.
        mode_text = self.training_mode_combo.currentText() # SeÃ§ilen eÄŸitim modunu al.
        training_mode = "human" if "human" in mode_text.lower() else "fast" # EÄŸitim modunu belirle.
        delay = self.training_speed_slider.value() / 1000.0 if hasattr(self, 'training_speed_slider') else 0.1 # CanlÄ± mod iÃ§in gecikme.
        
        self.info_panel.clear_training_progress()  # EÄŸitim baÅŸÄ±nda ilerleme bilgisini temizle.
        # Buton ve parametrelerin durumunu ayarla (eÄŸitim sÄ±rasÄ±nda Ã§oÄŸu pasif olur).
        self.train_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self.stop_game_button.setEnabled(False)
        self.set_game_buttons_enabled(False)
        self.set_params_enabled(False)
        # ---
        self.env.reset() # OrtamÄ± sÄ±fÄ±rla.
        self.training_rewards = []; self.training_steps = [] # Ã–dÃ¼l ve adÄ±m listelerini sÄ±fÄ±rla.
        # EÄŸitim thread'ini oluÅŸtur ve baÅŸlat.
        self.training_thread = TrainingThread(self.env, self.agent, episodes, mode=training_mode, delay=delay)
        self.training_thread.progress.connect(self.update_training_progress) # Ä°lerleme sinyaline baÄŸlan.
        self.training_thread.finished.connect(self.training_finished) # BitiÅŸ sinyaline baÄŸlan.
        self.training_thread.state_update.connect(self.update_training_visualization) # Durum gÃ¼ncelleme sinyaline baÄŸlan.
        self.training_thread.start() # Thread'i baÅŸlat.
        self.info_panel.set_status("EÄŸitim devam ediyor...")
        self.statusBar().showMessage(f"EÄŸitim baÅŸladÄ±. Toplam episode: {episodes}")
    def update_training_visualization(self):
        # EÄŸitim sÄ±rasÄ±nda arayÃ¼zÃ¼ gÃ¼nceller (Ã¶zellikle canlÄ± modda).
        self.update_ui()
    def update_training_progress(self, episode, reward, steps, epsilon):
        # EÄŸitim ilerlemesini alÄ±r ve arayÃ¼zde gÃ¶sterir.
        self.training_rewards.append(reward)
        self.training_steps.append(steps)
        # EÄŸitim bilgisi sadece eÄŸitim sÄ±rasÄ±nda gÃ¶sterilecek
        self.info_panel.set_training_progress(episode, self.episodes_spin.value(), reward, steps, epsilon)
        self.info_panel.update_info() # Bilgi panelini gÃ¼ncelle.
        self.update_ui() # Genel arayÃ¼zÃ¼ gÃ¼ncelle.
    def training_finished(self, rewards, steps):
        # EÄŸitim bittiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        # Buton ve parametrelerin durumunu eski haline getirir.
        self.train_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.stop_game_button.setEnabled(False)
        self.set_game_buttons_enabled(True)
        self.info_panel.clear_training_progress()  # EÄŸitim bitince eÄŸitim bilgisini gizle.
        # Son 100 bÃ¶lÃ¼mÃ¼n ortalama Ã¶dÃ¼l ve adÄ±m sayÄ±sÄ±nÄ± hesapla.
        avg_reward = sum(rewards[-100:]) / min(100, len(rewards)) if rewards else 0
        avg_steps = sum(steps[-100:]) / min(100, len(steps)) if steps else 0
        result_message = f"EÄŸitim tamamlandÄ±!\n\nToplam episode: {len(rewards)}\n"
        result_message += f"Son 100 episode ortalama Ã¶dÃ¼l: {avg_reward:.2f}\n"
        result_message += f"Son 100 episode ortalama adÄ±m: {avg_steps:.2f}\n\n"
        result_message += "Åimdi 'AI ile Oyna' butonunu kullanarak eÄŸitilen modeli test edebilirsiniz."
        QMessageBox.information(self, "EÄŸitim TamamlandÄ±", result_message) # Bilgilendirme mesajÄ± gÃ¶ster.
        self.statusBar().showMessage(f"EÄŸitim tamamlandÄ±! Son 100 episode ortalama Ã¶dÃ¼l: {avg_reward:.2f}, adÄ±m: {avg_steps:.2f}")
        self.training_thread = None
        self.model_trained = True # Model eÄŸitildi olarak iÅŸaretle.
        # self.training_status_label = QLabel("Model Durumu: EÄŸitildi"); self.training_status_label.setStyleSheet("color: green; font-weight: bold;") # Bu satÄ±r GUI'de bir yere eklenmeli.
        # EÄŸitim bitince parametreleri tekrar aktif et
        self.set_params_enabled(True)
        self.save_button.setEnabled(True) # Model eÄŸitildiÄŸi iÃ§in kaydet butonu aktif olur.
        self.ai_button.setEnabled(True) # Model eÄŸitildiÄŸi iÃ§in AI ile oyna butonu aktif olur.
        
    def stop_training(self):
        # EÄŸitimi durdurur.
        if self.training_thread:
            self.training_thread.stop() # EÄŸitim thread'ine durma sinyali gÃ¶nder.
            self.statusBar().showMessage("EÄŸitim durduruluyor...")
            self.set_params_enabled(True) # Parametreleri tekrar aktif et.
            # ButonlarÄ± da uygun ÅŸekilde ayarlamak gerekebilir.
            self.train_button.setEnabled(True)
            self.stop_button.setEnabled(False)
            self.set_game_buttons_enabled(True)

    def save_model(self):
        # EÄŸitilmiÅŸ DQN modelini kaydeder.
        save_dir = "models" # KayÄ±t dizini.
        if not os.path.exists(save_dir): os.makedirs(save_dir) # Dizin yoksa oluÅŸtur.
        # Dosya adÄ± iÃ§in zaman damgasÄ± ve grid boyutu kullanÄ±lÄ±r.
        timestamp = "dqn_model_" + str(self.grid_size) + "_" + str(random.randint(1000,9999)) + ".pth"
        filename, _ = QFileDialog.getSaveFileName(self, "DQN Modelini Kaydet", os.path.join(save_dir, timestamp), "PyTorch Files (*.pth);;All Files (*)") # KayÄ±t dialoÄŸu.
        if filename: # EÄŸer bir dosya adÄ± seÃ§ildiyse
            self.agent.save_model(filename) # AjanÄ±n modelini kaydet.
            self.statusBar().showMessage(f"DQN modeli kaydedildi: {filename}")

    def load_model(self):
        # KaydedilmiÅŸ bir DQN modelini yÃ¼kler.
        filename, _ = QFileDialog.getOpenFileName(self, "DQN Modeli YÃ¼kle", "models" if os.path.exists("models") else ".", "PyTorch Files (*.pth);;All Files (*)") # YÃ¼kleme dialoÄŸu.
        if filename: # EÄŸer bir dosya seÃ§ildiyse
            try:
                self.agent.load_model(filename) # AjanÄ±n modelini yÃ¼kle.
                self.model_loaded = True # Model yÃ¼klendi olarak iÅŸaretle.
                self.model_trained = True # YÃ¼klenen model eÄŸitilmiÅŸ sayÄ±lÄ±r.
                self.ai_button.setEnabled(True) # AI ile oyna butonunu aktif et.
                self.save_button.setEnabled(True) # YÃ¼klenen model kaydedilebilir.
                self.statusBar().showMessage(f"DQN modeli baÅŸarÄ±yla yÃ¼klendi: {filename}")
            except Exception as e:
                QMessageBox.critical(self, "Model YÃ¼kleme HatasÄ±", f"Model yÃ¼klenirken bir hata oluÅŸtu: {e}")
                self.model_loaded = False
                self.model_trained = False
                self.ai_button.setEnabled(False)
                self.save_button.setEnabled(False)

    def play_with_ai(self):
        # EÄŸitilmiÅŸ veya yÃ¼klenmiÅŸ model ile AI'Ä±n oynamasÄ±nÄ± baÅŸlatÄ±r.
        if self.game_timer.isActive(): # EÄŸer zamanlayÄ±cÄ± zaten aktifse (yani AI oynuyorsa)
            self.stop_game(); return # Oyunu durdur.
        if not self.model_trained and not self.model_loaded: # EÄŸer model yoksa
            QMessageBox.warning(self, "Model Yok", "AI ile oynamak iÃ§in Ã¶nce modeli eÄŸitmeniz veya yÃ¼klemeniz gerekiyor.")
            return
        self.env.reset(); self.game_mode = 'ai'; self.info_panel.set_status("AI ile oynanÄ±yor...")
        self.info_panel.clear_training_progress()  # AI ile oyna baÅŸlarken eÄŸitim bilgisini gizle.
        self.ai_episode_count = 1; self.ai_total_reward = 0 # AI bÃ¶lÃ¼m sayacÄ±nÄ± ve Ã¶dÃ¼lÃ¼nÃ¼ sÄ±fÄ±rla.
        self.game_timer.start(self.sim_speed) # Oyun zamanlayÄ±cÄ±sÄ±nÄ± baÅŸlat.
        # Buton ve parametrelerin durumunu ayarla.
        self.stop_game_button.setEnabled(True)
        self.train_button.setEnabled(False)
        self.stop_button.setEnabled(False)
        self.set_game_buttons_enabled(False) # DiÄŸer oyun butonlarÄ±nÄ± pasif yap.
        self.ai_button.setEnabled(False) # AI ile oyna butonu zaten basÄ±ldÄ±ÄŸÄ± iÃ§in pasif.
        self.human_button.setEnabled(False)
        self.reset_button.setEnabled(False)
        self.load_button.setEnabled(False)
        self.save_button.setEnabled(False)
        self.set_params_enabled(False) # Parametreleri pasif yap.

    def play_human_mode(self):
        # KullanÄ±cÄ±nÄ±n manuel olarak oynamasÄ±nÄ± saÄŸlar.
        if self.game_timer.isActive(): # EÄŸer AI oynuyorsa
            self.stop_game(); return # Oyunu durdur.
        self.env.reset(); self.game_mode = 'human'; # OrtamÄ± sÄ±fÄ±rla ve oyun modunu 'human' yap.
        self.info_panel.set_status("Manuel mod: Hareket=WASD/Ok TuÅŸlarÄ±, UÃ§/Kalk/Ä°n=Space, Kargo=E"); # KullanÄ±cÄ±ya bilgi ver.
        self.info_panel.clear_training_progress()  # Ä°nsan modunda eÄŸitim bilgisini gizle.
        self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
        # Buton ve parametrelerin durumunu ayarla.
        self.stop_game_button.setEnabled(True)
        self.set_game_buttons_enabled(False) # DiÄŸer oyun butonlarÄ±nÄ± pasif yap
        self.train_button.setEnabled(False)
        self.ai_button.setEnabled(False)
        self.human_button.setEnabled(False) # Manuel mod butonu zaten basÄ±ldÄ±ÄŸÄ± iÃ§in pasif.
        self.reset_button.setEnabled(False)
        self.load_button.setEnabled(False)
        self.save_button.setEnabled(False)
        self.set_params_enabled(False) # Parametreleri pasif yap.
        self.setFocus() # Klavye girdilerini almak iÃ§in pencereye odaklan.

    def keyPressEvent(self, event):
        # Klavye tuÅŸlarÄ±na basÄ±ldÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r (sadece manuel modda).
        if self.game_mode != 'human' or self.env.done: # EÄŸer manuel modda deÄŸilse veya bÃ¶lÃ¼m bittiyse bir ÅŸey yapma.
            return
        key = event.key() # BasÄ±lan tuÅŸu al.
        action = None # BaÅŸlangÄ±Ã§ta eylem yok.
        # WASD ve ok tuÅŸlarÄ± ile hareket
        if key in (Qt.Key_Down, Qt.Key_S): # AÅŸaÄŸÄ±
            action = 0
        elif key in (Qt.Key_Right, Qt.Key_D): # SaÄŸa
            action = 1
        elif key in (Qt.Key_Up, Qt.Key_W): # YukarÄ±
            action = 2
        elif key in (Qt.Key_Left, Qt.Key_A): # Sola
            action = 3
        # Kargo al/bÄ±rak: E
        elif key == Qt.Key_E:
            action = 4
        # Kalk/Ä°n: Space
        elif key == Qt.Key_Space:
            action = 5
        
        if action is not None: # EÄŸer geÃ§erli bir eylem tuÅŸuna basÄ±ldÄ±ysa
            _, _, done, info = self.env.step(action) # Eylemi uygula.
            self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            if "action" in info and info["action"]: # EÄŸer eylemle ilgili bir mesaj varsa durum Ã§ubuÄŸunda gÃ¶ster.
                self.statusBar().showMessage(info["action"])
            if done: # EÄŸer bÃ¶lÃ¼m bittiyse
                self.info_panel.set_status("Oyun bitti! Manuel modda yeni oyun iÃ§in 'SÄ±fÄ±rla' veya 'Oyunu Durdur' kullanÄ±n.")
                # Oyun bittiÄŸinde bazÄ± butonlarÄ± tekrar aktif hale getirebiliriz.
                self.stop_game_button.setEnabled(False) # Oyunu durdur butonu pasif.
                self.reset_button.setEnabled(True) # SÄ±fÄ±rla butonu aktif.
                self.human_button.setEnabled(True) # Tekrar manuel oynamak iÃ§in.
                # DiÄŸer butonlar da duruma gÃ¶re ayarlanabilir.

    def stop_game(self):
        # AI veya manuel oyunu durdurur.
        if self.game_timer.isActive() or self.game_mode == 'human': # EÄŸer AI oynuyorsa veya manuel moddaysa
            self.game_timer.stop(); self.game_mode = None # ZamanlayÄ±cÄ±yÄ± durdur ve oyun modunu sÄ±fÄ±rla.
            self.info_panel.set_status("Oyun durduruldu.")
            self.info_panel.clear_training_progress()  # Oyun durunca eÄŸitim bilgisini gizle.
            self.statusBar().showMessage("Oyun durduruldu.")
            # Buton ve parametrelerin durumunu eski haline getir.
            self.stop_game_button.setEnabled(False)
            self.stop_button.setEnabled(False) # EÄŸitim durdurma butonu da pasif olmalÄ±.
            self.set_game_buttons_enabled(True) # Oyunla ilgili ana butonlarÄ± aktif et.
            self.set_params_enabled(True) # Parametreleri aktif et.
# =====================
# Ana Uygulama BaÅŸlatÄ±cÄ±
# =====================
if __name__ == "__main__":
    # PyQt5 uygulamasÄ±nÄ± baÅŸlatÄ±r.
    app = QApplication(sys.argv)
    window = DroneDeliverySimulator() # Ana pencereyi oluÅŸtur.
    window.show() # Pencereyi gÃ¶ster.
    sys.exit(app.exec_()) # Uygulama dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlat ve Ã§Ä±kÄ±ÅŸta temizle.